---
title: "DATA_PROCESSING"
author: "Ruihang_Han"
date: "2025-04-21"
output: html_document
---


```{r}
# Load required libraries
library(tidyverse)
library(readr)
library(stringr)
library(fs)
library(furrr)    # Parallel map
library(future)   # Plan and manage workers

# Step 1: Define file paths
metadata_path <- "metadata/training_metadata.csv"
tsv_dir <- "train_tsv"
output_path_csv <- "train_df.csv"
output_path_rds <- "train_df.rds"

# Step 2: Enable parallel processing (use all available cores)
plan(multisession, workers = parallel::detectCores())

# Step 3: Load training metadata
train_meta <- read_csv(metadata_path, show_col_types = FALSE)

# Step 4: Identify valid .tsv files that match metadata participant IDs
tsv_files_all <- dir_ls(tsv_dir, glob = "*.tsv")
valid_files <- tsv_files_all[
  map_chr(tsv_files_all, ~ str_extract(basename(.x), "(?<=sub-)[^_]+")) %in% train_meta$participant_id
]

# Step 5: Define function to extract upper triangle of connectome matrix
extract_tsv_row <- function(file) {
  pid <- str_extract(basename(file), "(?<=sub-)[^_]+")
  mat <- tryCatch(
    as.matrix(read_tsv(file, col_names = FALSE, progress = FALSE)),
    error = function(e) return(NULL)
  )
  if (is.null(mat)) return(NULL)
  vec <- mat[upper.tri(mat)]
  tibble(participant_id = pid,
         !!!set_names(as.list(vec), paste0("V", seq_along(vec))))
}

# Step 6: Apply feature extraction in parallel
train_features <- future_map(valid_files, extract_tsv_row, .progress = TRUE) %>%
  compact() %>%
  bind_rows()

# Step 7: Merge extracted features with metadata
train_df <- train_features %>%
  inner_join(train_meta, by = "participant_id")

# Step 8: Save final output as CSV and RDS
write_csv(train_df, output_path_csv)
saveRDS(train_df, output_path_rds)

cat("✅ Training data processing complete —", nrow(train_df), "records saved to:\n",
    "→ ", output_path_csv, "\n",
    "→ ", output_path_rds, "\n")
```